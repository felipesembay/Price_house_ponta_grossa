# Scripts de Web Scraping - ZapImÃ³veis

## ğŸ“‹ VisÃ£o Geral

Este diretÃ³rio contÃ©m scripts para fazer web scraping do site ZapImÃ³veis e processar os dados coletados.

## ğŸš€ Scripts DisponÃ­veis

### 1. `scraper_robusto.py` - Coletor Principal

Script principal para fazer scraping de imÃ³veis com estratÃ©gia anti-bloqueio.

**LocalizaÃ§Ã£o**: `src/scraper_robusto.py`

**Como usar**:

```bash
conda activate regression
python src/scraper_robusto.py
```

**Inputs solicitados**:
- **Cidade**: Nome da cidade (ex: guarapuava, curitiba)
- **Estado**: Sigla do estado (ex: pr, sp)
- **NÃºmero de pÃ¡ginas**: 
  - Digite um nÃºmero especÃ­fico (ex: 12)
  - Pressione ENTER para coletar atÃ© acabar

**SaÃ­das**:
- **Arquivos individuais**: `data/raw/por_pagina/cidade_estado_paginaN.csv`
- **Arquivo consolidado**: `data/raw/imoveis_cidade_TIMESTAMP.csv`

**CaracterÃ­sticas**:
- âœ… Fecha/reabre navegador entre pÃ¡ginas (evita bloqueio)
- âœ… Delays randomizados (5-10s)
- âœ… RotaÃ§Ã£o de User-Agents
- âœ… DetecÃ§Ã£o automÃ¡tica de fim de pÃ¡ginas
- âœ… Salva cada pÃ¡gina individualmente

---

### 2. `unir_arquivos.py` - Consolidador de Dados

Script para unir mÃºltiplos arquivos CSV em um Ãºnico DataFrame.

**LocalizaÃ§Ã£o**: `data/unir_arquivos.py`

**Como usar**:

```bash
cd data
conda activate regression
python unir_arquivos.py
```

**OpÃ§Ãµes**:

#### OpÃ§Ã£o 1: Unir arquivos de uma cidade especÃ­fica
```
Escolha: 1
Cidade: guarapuava
Estado: pr
```

**Resultado**: `data/raw/imoveis_guarapuava_pr_completo.csv`

#### OpÃ§Ã£o 2: Unir TODOS os arquivos
```
Escolha: 2
```

**Resultado**: `data/raw/imoveis_todos_completo.csv`

**Funcionalidades**:
- Remove duplicatas automaticamente (baseado no link)
- Mostra estatÃ­sticas por cidade
- Valida integridade dos dados

---

## ğŸ“ Estrutura de Arquivos

```
data/
â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ por_pagina/              # Arquivos individuais por pÃ¡gina
â”‚   â”‚   â”œâ”€â”€ guarapuava_pr_pagina1.csv
â”‚   â”‚   â”œâ”€â”€ guarapuava_pr_pagina2.csv
â”‚   â”‚   â”œâ”€â”€ curitiba_pr_pagina1.csv
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ imoveis_guarapuava_pr_completo.csv    # Consolidado por cidade
â”‚   â””â”€â”€ imoveis_todos_completo.csv             # Consolidado geral
â””â”€â”€ unir_arquivos.py             # Script de uniÃ£o
```

---

## ğŸ’¡ Exemplos de Uso

### Exemplo 1: Coletar 12 pÃ¡ginas de Guarapuava

```bash
python src/scraper_robusto.py

# Inputs:
# Cidade: guarapuava
# Estado: pr
# PÃ¡ginas: 12
```

**Resultado**:
- 12 arquivos: `guarapuava_pr_pagina1.csv` atÃ© `guarapuava_pr_pagina12.csv`
- 1 arquivo consolidado: `imoveis_guarapuava_TIMESTAMP.csv`

### Exemplo 2: Coletar atÃ© acabar (Curitiba)

```bash
python src/scraper_robusto.py

# Inputs:
# Cidade: curitiba
# Estado: pr
# PÃ¡ginas: [ENTER]
```

**Resultado**: Coleta automaticamente atÃ© encontrar 2 pÃ¡ginas vazias consecutivas

### Exemplo 3: Unir dados de Guarapuava

```bash
cd data
python unir_arquivos.py

# OpÃ§Ã£o: 1
# Cidade: guarapuava
# Estado: pr
```

**Resultado**: `data/raw/imoveis_guarapuava_pr_completo.csv`

### Exemplo 4: Unir dados de mÃºltiplas cidades

```bash
# 1. Coletar Guarapuava
python src/scraper_robusto.py
# Cidade: guarapuava, Estado: pr, PÃ¡ginas: 12

# 2. Coletar Curitiba
python src/scraper_robusto.py
# Cidade: curitiba, Estado: pr, PÃ¡ginas: 20

# 3. Unir tudo
cd data
python unir_arquivos.py
# OpÃ§Ã£o: 2
```

**Resultado**: `data/raw/imoveis_todos_completo.csv` com dados de ambas as cidades

---

## ğŸ”§ ConfiguraÃ§Ãµes AvanÃ§adas

### Ajustar delays entre pÃ¡ginas

Edite `src/scraper_robusto.py`:

```python
# Linha ~550
DELAY_MIN = 5.0   # MÃ­nimo 5 segundos
DELAY_MAX = 10.0  # MÃ¡ximo 10 segundos
```

### Modo headless (sem interface grÃ¡fica)

```python
# Linha ~552
HEADLESS = True
```

### Desabilitar salvamento por pÃ¡gina

```python
# Linha ~560
salvar_por_pagina=False
```

---

## ğŸ“Š Formato dos Dados

Cada arquivo CSV contÃ©m as seguintes colunas:

| Coluna | Tipo | DescriÃ§Ã£o | Exemplo |
|--------|------|-----------|---------|
| `preco` | string | PreÃ§o do imÃ³vel | "R$ 350.000" |
| `rua` | string | Nome da rua | "Rua dos Garis" |
| `endereco` | string | EndereÃ§o/descriÃ§Ã£o | "Casa...Dos Estados, Guarapuava" |
| `quartos` | float | NÃºmero de quartos | 2.0 |
| `banheiros` | float | NÃºmero de banheiros | 2.0 |
| `area_m2` | float | Ãrea em mÂ² | 65.0 |
| `link` | string | URL do anÃºncio | "https://..." |
| `cidade` | string | Cidade | "guarapuava" |
| `estado` | string | Estado | "pr" |
| `data_coleta` | string | Data/hora da coleta | "2026-02-06 16:34:38" |

---

## ğŸ› Troubleshooting

### Problema: Scraper sendo bloqueado

**SoluÃ§Ã£o**: Aumentar delays
```python
DELAY_MIN = 10.0
DELAY_MAX = 20.0
```

### Problema: Arquivos nÃ£o encontrados ao unir

**SoluÃ§Ã£o**: Verificar caminho
```bash
ls data/raw/por_pagina/
```

### Problema: Muitas duplicatas

**SoluÃ§Ã£o**: O script `unir_arquivos.py` jÃ¡ remove duplicatas automaticamente

---

## ğŸ“ PrÃ³ximos Passos

ApÃ³s coletar e consolidar os dados:

1. **Limpeza**: Converter preÃ§os para float, padronizar endereÃ§os
2. **Feature Engineering**: Extrair bairro, calcular preÃ§o/mÂ²
3. **Modelagem**: Treinar modelo de regressÃ£o para prever preÃ§os

---

## âœ… Checklist de Uso

- [ ] Ativar ambiente conda: `conda activate regression`
- [ ] Executar scraper: `python src/scraper_robusto.py`
- [ ] Informar cidade, estado e nÃºmero de pÃ¡ginas
- [ ] Aguardar conclusÃ£o (pode levar vÃ¡rios minutos)
- [ ] Verificar arquivos em `data/raw/por_pagina/`
- [ ] Unir arquivos: `python data/unir_arquivos.py`
- [ ] Verificar arquivo consolidado em `data/raw/`
